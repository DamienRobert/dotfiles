If T(n)=aT(n/b)+O(n^d), alors
- T(n)=O(n^d) si d>log_b a
- T(n)=O(n^d log n) si d=log_b a
- T(n)=O(n^{log_b a}) si d<log_b a

Pr: cf http://www.cs.berkeley.edu/~vazirani/algorithms/chap2.pdf
On a un arbre de profondeur log_b n, et de largeur a^{log_b n}=n^{log_b a}
Au niveau k, on a a^k problèmes de complexités O(n/b^k)^d.
D'où un coût total de  n^d \sum_k O((a/b^d)^k); k<=log_b n
Si a/b^d < 1, le coût est O(du premier terme)=O(n^d)
Si a/b^d > 1 le coût est donné par O(du dernier terme)=O(n^d (a/b^d)^{log_b n})
   =O(n^{log_b a})
Si a/b^d=1 le coût est donné par O(n^d \log_b n)

En résumé:
il y a O(n^{log_b a}) noeuds; donc soit le nombre de noeus domine le coût
O(n^d) de reconstruction et c'est explorer l'arbre qui domine, soit le coût 
de reconstruction domine le nombre de noeuds et c'est la dernière
reconstruction qui domine, soit c'est pareil et on prend un facteur log n.
